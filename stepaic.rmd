---
output: word_document
---
```{r}
library(RCurl)
library(MASS)
library(leaps)
```

```{r}
houseData <- read.csv("file:///G:/Ryerson-BigData/capstone-R/CKME-136/data/kc_house_data.csv")
houseData$date<-NULL
houseData$id<-NULL
colnames(houseData)
```

##Let us now use the forward selection algorithm using stepAIC.
```{r echo=TRUE}
full = lm(price~.,data=houseData)
null = lm(price~1,data=houseData)
stepF = stepAIC(null, scope=list(lower=null, upper=full), direction='forward', trace=TRUE)
summary(stepF)

subsets = regsubsets(price~.,data=houseData,nbest=1,)
sub.sum = summary(subsets)
as.data.frame(sub.sum$outmat)

```


##Let us now use the backward selection algorithm using stepAIC.
```{r echo=TRUE}
full = lm(price~.,data=houseData)
null = lm(price~1,data=houseData)
stepF = stepAIC(full, direction= 'backward', trace=TRUE)
summary(stepF)

subsets = regsubsets(price~.,data=houseData,nbest=1,)
sub.sum = summary(subsets)
as.data.frame(sub.sum$outmat)

```

```{r echo=TRUE}
#since floor is having very less contribution to price.we are not including floor in our model.
str(houseData)
sapply(houseData, is.numeric)
houseDatas <- houseData[ , sapply(houseData, is.numeric)]
cor(houseDatas)
```

## Using Step AIC we got below variables needs to part of model
## 1:sqft_living 
## 2:lat 
## 3:grade 
## 4:yr_built 
## 5:waterfront 
## 6:bedrooms 
## 7:bathrooms 
## 8:zipcode 
## 9:long

```{r echo=TRUE}
#since no any variable is highly correlated with each other
houseData$floors<-NULL
houseData$date<-NULL
colnames(houseData)
```

```{r echo=TRUE}
newhouseData <- subset(houseData, select  = c(price,sqft_living,lat,view,grade,yr_built,waterfront,bedrooms,bathrooms,zipcode,long))

set.seed(1)
i=0.6
storage <- list(c(), c(), c(),c())
for(i in seq(from=0.6, to=0.9, by=0.01)){
  rn_train <- sample(nrow(newhouseData),floor(nrow(newhouseData)*i))
  train <- newhouseData[rn_train,colnames(newhouseData)]
  test <- newhouseData[-rn_train,colnames(newhouseData)]
  model<-lm(price~sqft_living+lat+view+grade+yr_built+waterfront+bedrooms+bathrooms+zipcode+long,data = train)
  prediction <- predict(model,interval='prediction',newdata = test)
  train_prediction = fitted(model)
  train_rmse = sqrt(sum((train_prediction-train$price)^2)/nrow(train))
  test_rmse = sqrt(sum((prediction - test$price)^2)/nrow(test))
  storage[[1]]<-c(storage[[1]],i)
  storage[[2]]<-c(storage[[2]],test_rmse)
  storage[[3]]<-c(storage[[3]],train_rmse)
 
}

##find the LM with minimun training error
RMSE = storage[[3]]
minimumVal = min(RMSE)
minimumVal
indx = which(RMSE==min(RMSE))
indx
storage[[1]][indx]

cat("\nMinimum Training RMSE of Regression:",storage[[3]][indx],"\nRMSE of testing :",storage[[2]][indx], "\nTraining data Percentage:",storage[[1]][indx])

```


## Now we come to a conclusion that 73% Training data provides the Minimum RMSE
## 1: SET training Data = 73% &
## 2: Get model with coeeficient & Intercept
## 3: Draw the error of Histogram to get confidence with model
## 4: Find out how many data have less than 25% of error
```{r}
set.seed(1)
rn_train <- sample(nrow(newhouseData),floor(nrow(newhouseData)*storage[[1]][indx]))
train <- newhouseData[rn_train,colnames(newhouseData)]
test <- newhouseData[-rn_train,colnames(newhouseData)]
modelXGen <- lm(price~sqft_living+lat+view+grade+yr_built+waterfront+bedrooms+bathrooms+zipcode+long,data = train)
summary(modelXGen)

predictionXGen <- predict(modelXGen,interval='prediction',newdata = test)
test_rmseXGen = sqrt(sum((predictionXGen - test$price)^2)/nrow(test))
errors <- predictionXGen[,'fit'] - test$price
hist(errors,col=(c("gold","darkgreen")))
rel_change = 1 - ((test$price - abs(errors)) / test$price)
##Now the percentage of cases with less than 25% error.
pred25 = table(rel_change<0.25)["TRUE"] / nrow(test)
pred25

cat("\nConclusion:percent of data having less than 25% error:",pred25)
```

