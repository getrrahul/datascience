---
output: word_document
---
```{r}
install.packages('plotly', repos = 'http://cran.us.r-project.org')
library(ggplot2)
library(GGally)

library(RCurl)
library(MASS)
library(leaps)

install.packages("Boruta", repos = "http://cran.us.r-project.org")
library(Boruta)

```


```{r echo=TRUE}
checkNaFunction <- function(houseData){
naColumns <- c()
#checking NA for each columns
for(i in 1:ncol(houseData)) {
  #cat(sprintf("Checking NA: %s \n", colnames(houseData)[i]))
  if(length(which(is.na(houseData[,i]))) > 0){
    #cat(sprintf("There is NA: %s \n" , colnames(houseData)[i]))
    naColumns <- c(naColumns, colnames(houseData)[i])
  }
}
return(naColumns)
}

```


```{r echo=TRUE}
bucketByColumn <- function(houseData,i){
minP <- min(as.numeric(houseData[,i]))
maxP <- max(as.numeric(houseData[,i]))
rangeP <- range(as.numeric(houseData[,i]))
rangeP
cat(sprintf("Min-Max value for: %s , MAX: %d, MIN: %d \n", colnames(houseData)[i], maxP, minP))
}

```

```{r}
findSSEByColName <- function(colName, SSEVals){
  modlm<-lm(as.formula(paste("log(price)~", paste(c(colName), collapse="+"))),data=train)
  ## Predicting prices using each Model. we need to take exponent of predict function since it returns log of price.
  predt<-exp(predict(modlm,newdata=test))
  SSE<-sum((test$price-predt)^2)
  SSEVals[[1]]<-c(SSEVals[[1]],colName)
  SSEVals[[2]]<-c(SSEVals[[2]],SSE)
  return(SSEVals)
}
```


```{r echo=TRUE}
analysis <- function(houseData, i, labels, plotLog, boxLog){
  #Simple Plot with Price
  plot(houseData[,i], houseData$price, main = labels[1], xlab = labels[2],ylab = labels[3], col=(c("gold","darkgreen")))
  #Plot with Log(Price )
  plot(houseData[,i],log(houseData$price), main = labels[1], xlab =labels[2], ylab = paste('Log of ', labels[3]), col=(c("gold","darkgreen")))
  
  if(plotLog=='Y'){
      #Plot with Log(Price ) & Log(independent-variable)
      plot(log(houseData[,i]+0.5),log(houseData$price), main = labels[1], xlab = paste('Log of ',labels[2]), ylab = paste('Log of ', labels[3]), col=(c("gold","darkgreen")))
  }
  #Histogram of independent-variable
  hist(houseData[,i], main = labels[2], xlab = labels[2], ylab = "Frequency", col=(c("gold","darkgreen")))
  #BoxPlot of independent-variable
  boxplot(houseData[,i], main = labels[2], xlab = labels[2], ylab= "Frequency", col=(c("gold","darkgreen")))
  if(boxLog=='Y'){
    #BoxPlot of independent-variable & dependent variable
    boxplot(houseData$price~houseData[,i], main = labels[1], xlab = labels[2], ylab= labels[3], col=(c("gold","darkgreen")))
    #BoxPlot of independent-variable & dependent variable
    boxplot(log(houseData$price)~houseData[,i], main = labels[1], xlab = labels[2], ylab=  paste('Log of ', labels[3]), col=(c("gold","darkgreen")))
    
      #BoxPlot of independent-variable & dependent variable
      boxplot(log(houseData$price)~log(houseData[,i]+0.5), main = labels[1], xlab = paste('Log of ',labels[2]), ylab=  paste('Log of ', labels[3]), col=(c("gold","darkgreen")))
  }
  #Corelation of independent-variable with Price
  cor(houseData[,i],houseData$price)
}

```


#Data Importing And Cleaning
```{r echo=TRUE}
houseData <- read.csv("file:///G:/Ryerson-BigData/capstone-R/CKME-136/data/kc_house_data.csv")
head(houseData)
colnames(houseData)
naColumns <- checkNaFunction(houseData)
if(length(naColumns)>0){
  cat("Found NA Colums:")
  for(i in 1:length(naColumns)) {
    cat(sprintf("%s,", colnames(houseData)[i]))
  }
}

cat ("Conclusion: Here we conclude that this data does not hold any column with NA.")
##bucketByColumn(houseData,3)

```

## #################################################
## ###### stepByStep Analysis - START ######
## #################################################

#Price with other attributes
```{r echo=TRUE}
## verify the relationship between price, bedrooms, bathrooms, sqft_living and sqft lot
plot1 <- ggpairs(data=houseData, columns=3:7, mapping = aes(color = "dark green"), axisLabels="show")
plot1

## verify the relationship between price, floors, waterfront, view, condition and grade
plot2 <- ggpairs(data=houseData, columns=c(3,8:12), mapping = aes(color = "dark green"), axisLabels="show")
plot2

## verify the relationship between price, yr built, lat and long
plot3 <- ggpairs(data=houseData, columns=c(3,15,18,19), mapping = aes(color = "dark green"), axisLabels="show")
plot3

```


##Correlation among all the variables
```{r echo=TRUE}
#Remove the columns which does not hold any significance in predicing house price
houseData$date <- NULL
houseData$id <- NULL
cor(houseData)
cat ("\nConclusion: sqft_living , sqft_above, grade, sqft_living15, bathrooms have moderate to strong correlation with Price")
cat ("\nConclusion: bathrooms has  moderate to strong correlation sqft_living, sqft_above, grade, sqft_living15")
cat ("\nConclusion: sqft_living has  moderate to strong correlation sqft_above, grade, sqft_living15  and all the above variables studied.")
cat ("\nConclusion: grade has  moderate to strong correlation sqft_above, sqft_living15  and all the above variables studied.")
cat ("\nConclusion: sqft_above has  moderate to strong correlation sqft_living15 and all the above variables studied.")
cat ("\nConclusion: sqft_basement has moderate correlation with sqft_living only.")
cat ("\nConclusion: yr_built has moderate correlation with bathrooms, floors, grade, sqft_above only.")
cat ("\nConclusion: sqft_lot15 has strong correlation with sqft_lot only.")
cat ("\nConclusion: waterfront, view, condition, zipcode, latitude, longitude, yr_renovated has  very weak with other attributes as well as with price.")
cat ("\nConclusion: we can see that zip-code has very weak co-orelation -0.053202854, so let us remove it")
houseData$zipcode <- NULL
 houseData <- read.csv("file:///G:/Ryerson-BigData/capstone-R/CKME-136/data/kc_house_data.csv")

```

#Now Let us do analysis of price with all other variables one by one

#Bedroom Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,4)
analysis(houseData,4,c('Bedrooms vs. price','Bedrooms', 'Price of House'), 'Y', 'Y')
#*******Removing the outliers
#Since more than 7 bedrooms are very rare.Also it's the outlier for my model.
#I  have removed  the outlier data.
houseData<-subset(houseData,bedrooms>=1 & bedrooms<=7)
#*******Once we removed the outliers, again get the analysis
analysis(houseData,4,c('Bedrooms vs. price','Bedrooms', 'Price of House'), 'Y', 'Y')
#bucketByColumn(houseData,4)
cat ("Conclusion: here we can found that log of bedroom give better performance than bedroom.")
```


#Bathroom Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,5)
analysis(houseData,5,c('Bathrooms vs. price','Bathrooms', 'Price of House'), 'Y', 'Y')
cat ("Conclusion: here we found that log of bathrooms give better performance than bathrooms with one expection in when bathroom=7.")
#*******Removing the outliers
#More than 4 bathrooms are very rare in this data.So I am removing it.
houseData<-subset(houseData,bathrooms>=1 & bathrooms<=4)
analysis(houseData,5,c('Bathrooms vs. price','Bathrooms', 'Price of House'), 'Y', 'Y')
#bucketByColumn(houseData,5)
```


#SQFT Living Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,6)
analysis(houseData,6,c('Living-Sqft vs. price','Living-Sqft', 'Price of House'), 'Y', 'N')
cat ("Conclusion: There is Strong correlation between sqft_living and price as sqft_living increases, price increases as well.")
#*******Removing the outliers
houseData<-subset(houseData,sqft_living >1000 & sqft_living<=4000)
analysis(houseData,6,c('Living-Sqft vs. price','Living-Sqft', 'Price of House'), 'Y', 'N')
#bucketByColumn(houseData,6)
```


## SQFT_LOT Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,7)
analysis(houseData,7,c('LotSize vs. price','LotSize', 'Price of House'), 'Y', 'N')
#*******Removing the outliers
houseData<-subset(houseData,sqft_lot>=0 & sqft_lot<=25000)
analysis(houseData,7,c('LotSize vs. price','LotSize', 'Price of House'), 'Y', 'N')
#bucketByColumn(houseData,7)
```

## FLOOR Vs Price analysis
```{r echo=TRUE}
analysis(houseData,8,c('Floors vs. price','Floor', 'Price of House'), 'Y', 'Y')
##bucketByColumn(houseData,8)
```

## SQFT_LOT Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,9)
analysis(houseData,9,c('WaterFront vs. price','WaterFront', 'Price of House'), 'Y', 'N')
#*******Removing the outliers
houseData<-subset(houseData,sqft_lot>=0 & sqft_lot<=25000)
analysis(houseData,9,c('WaterFront vs. price','WaterFront', 'Price of House'), 'Y', 'N')
#bucketByColumn(houseData,9)
```


## View Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,10)
analysis(houseData,10,c('View vs. price','View', 'Price of House'), 'Y', 'Y')
cat ("Conclusion: Here we can se that as view increases price also increases as well")
#*******Removing the outliers
analysis(houseData,10,c('View vs. price','View', 'Price of House'), 'Y', 'Y')
#bucketByColumn(houseData,10)
```


##CONDITION Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,11)
analysis(houseData,11,c('Condition vs. price','condition', 'Price of House'), 'Y', 'Y')
#*******Removing the outliers
houseData<-subset(houseData,condition>=3& condition<=5)
analysis(houseData,11,c('Condition vs. price','condition', 'Price of House'), 'Y', 'Y')
#bucketByColumn(houseData,11)
```


##Grade Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,12)
analysis(houseData,12,c('Grade vs. price','Grade', 'Price of House'), 'Y', 'Y')
cat ("Conclusion: Here we can se that as grade increases price also increases as well")
#*******Removing the outliers
#Most of the houses grades are between 6-10 
houseData<-subset(houseData,grade >= 6 & grade<=10)
analysis(houseData,12,c('Grade vs. price','Grade', 'Price of House'), 'Y', 'Y')
#bucketByColumn(houseData,12)
#grade is good without log 
```

#SQFT_ABOVE Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,13)
analysis(houseData,13,c('MainHouseSize vs. price','MainHouseSize', 'Price of House'), 'Y', 'N')
#*******Removing the outliers
houseData<-subset(houseData,sqft_above >=500 & sqft_above<=3500)
analysis(houseData,13,c('MainHouseSize vs. price','MainHouseSize', 'Price of House'), 'Y', 'N')
#bucketByColumn(houseData,13)

```


##SQFT_BASEMENT Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,14)
analysis(houseData,14,c('BasementSize vs. price','BasementSize', 'Price of House'), 'Y', 'N')
#*******Removing the outliers
houseData<-subset(houseData,sqft_basement >=0 & sqft_basement<=1500)
analysis(houseData,14,c('BasementSize vs. price','BasementSize', 'Price of House'), 'Y', 'N')
#bucketByColumn(houseData,14)
```


#YR_BUILT Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,15)
analysis(houseData,15,c('YearBuilt vs. price','YearBuilt', 'Price of House'), 'Y', 'N')
#*******Removing the outliers
#houseData<-subset(houseData,yr_built>=1950& yr_built<=2015)
#analysis(houseData,15,c('YearBuilt vs. price','YearBuilt', 'Price of House'), 'Y', 'N')
#bucketByColumn(houseData,15)
```

#YR_RENOVATED Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,16)
analysis(houseData,16,c('YearRenovated vs. price','YearRenovated', 'Price of House'), 'N', 'N')
#*******Removing the outliers
#houseData<-subset(houseData,yr_renovated>=1950& yr_renovated<=2015)
#analysis(houseData,16,c('YearRenovated vs. price','YearRenovated', 'Price of House'), 'N')
#bucketByColumn(houseData,16)
```


#ZIPCODE Vs Price analysis
```{r echo=TRUE}
analysis(houseData,17,c('ZipCode vs. price','ZipCode', 'Price of House'), 'Y', 'N')
```


##LAT Vs Price analysis
```{r echo=TRUE}
analysis(houseData,18,c('Latitude vs. price','latitude', 'Price of House'), 'N', 'N')
cat ("Conclusion: Here we can see that LAThas normal dist relationship but not strong. So we should include LAT for price predication")
#*******Removing the outliers
houseData<-subset(houseData,lat>=47.3)
analysis(houseData,18,c('Latitude vs. price','latitude', 'Price of House'), 'N', 'N')
```

## LONG Vs Price analysis
```{r echo=TRUE}
analysis(houseData,19,c('Longitude vs. price','Longitude', 'Price of House'), 'N', 'N')

```


## SQFT_LIVING15 Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,20)
analysis(houseData,20,c('sqft_living15 vs. price','sqft_living15', 'Price of House'), 'Y', 'N')

```

##SQFT_LOT15 Vs Price analysis
```{r echo=TRUE}
#bucketByColumn(houseData,21)
analysis(houseData,21,c('sqft_lot15 vs. price','sqft_lot15', 'Price of House'), 'Y', 'N')
#*******Removing the outliers
houseData<-subset(houseData,sqft_lot15>=0 & sqft_lot<=20000)
analysis(houseData,21,c('sqft_lot15 vs. price','sqft_lot15', 'Price of House'), 'Y', 'N')
#bucketByColumn(houseData,21)

```



##Correlation among all the variables
```{r echo=TRUE}
#houseData <- read.csv("file:///G:/Ryerson-BigData/capstone-R/CKME-136/data/kc_house_data.csv")
#Remove the columns which does not hold any significance in predicing house price
houseData$date <- NULL
#houseData$id <- NULL
#houseData$zipcode <- NULL
cor(houseData)
cat ("\nConclusion: sqft_living , sqft_above, grade, sqft_living15, bathrooms have moderate to strong correlation with Price")
cat ("\nConclusion: bathrooms has  moderate to strong correlation sqft_living, floors, sqft_above, grade, sqft_living15, bedrooms")
cat ("\nConclusion: sqft_living has  moderate to strong correlation sqft_above, grade, sqft_living15, bedrooms  and all the above variables studied.")
cat ("\nConclusion: grade has  moderate to strong correlation sqft_above, sqft_living15  and all the above variables studied.")
cat ("\nConclusion: sqft_above has  moderate to strong correlation sqft_living15 and all the above variables studied.")
cat ("\nConclusion: sqft_basement has moderate correlation with sqft_living only.")
cat ("\nConclusion: yr_built has moderate correlation with bathrooms, floors, grade, sqft_above only.")
cat ("\nConclusion: sqft_lot15 has strong correlation with sqft_lot only.")
cat ("\nConclusion: waterfront, view, condition, zipcode, latitude, longitude, yr_renovated has  very weak with other attributes as well as with price.")


```

##Start with price & sqft_living for Relationship
```{r echo=TRUE}
analysis(houseData,6,c('Living-Sqft vs. price','Living-Sqft', 'Price of House'), 'Y', 'N')

## Since above scatterplot is too crowded, I will plot aggregated vectors to verify the relationship between 2 (price, sqft_living) variables. 
vec_price_sqftliving <-aggregate(price~sqft_living, FUN=mean, data=houseData)
plot(vec_price_sqftliving, col=(c("gold","darkgreen")))
scatterplot1<-recordPlot()

cat ("\nConclusion: Plot does not show that price and sqft_living are linearly related. It looks like an exponential relationship.")

plot(log(vec_price_sqftliving$sqft_living),log(vec_price_sqftliving$price), main="Log of Sqft_Living vs. Log of Price of House", xlab="Log Sqft_Living", ylab="Log Price of House", col=(c("gold","darkgreen")))
scatterplot2<-recordPlot()

cat ("\nConclusion: Relationship between variables in plot 1 seems to be exponential and in plot 2 it seems to be linear.")
```


## My fisrt 5 variables are: sqft_living, bathrooms, grade, view, sqft_living15, sqft_above.
## Each of box plots shows that above variables might be directly related in predicting house prices.

## To support my finding, I also computed correlation between prices and variables, and my top 5 picks are supported with correlation coefficients as well [see below]

# corr between price vs sqft_living: 0.70203505
# corr between price vs bathrooms: 0.52513751
# corr between price vs bedrooms: 0.308349598
# corr between price vs floors: 0.256793888
# corr between price vs waterfront: 0.266369434
# corr between price vs view: 0.397293488
# corr between price vs grade: 0.66743426
# corr between price vs sqft_above: 0.6055672984
# corr between price vs lat: 0.3070034800
# corr between price vs sqft_living15: 0.58537890



## Prepare the data for the Model
## 1: seed is being set, so that distribution is same at each run
## 2: put 70% to train-set
## 3: put 30% to test-set
```{r echo=TRUE}
set.seed(1)
rn_train <- sample(nrow(houseData),floor(nrow(houseData)*0.70))
train <- houseData[rn_train,colnames(houseData)]
test <- houseData[-rn_train,colnames(houseData)]
```

## Selection Method: START
## We have suggested 12 variables. I will calculate SSE and see which one gives me a smaller SSE and I pick that variable.
## From above correlation, variables selected are:  bedrooms, bathrooms, log(sqft living), log(sqft lot), floors, waterfront, view, grade, yr built, lat

## #################################################
## ###### stepByStep Analysis - Step -01 ######
## ######  One variable predication - START ######
## #################################################

```{r echo=TRUE}
## Creating Models using 1 variables fpr each of the variables with price, so total we have 12 Models. 
SSEVals <- list(c(), c())
SSEVals = findSSEByColName('bedrooms', SSEVals)
SSEVals = findSSEByColName('bathrooms', SSEVals)
SSEVals = findSSEByColName('log(sqft_living)', SSEVals)
SSEVals = findSSEByColName('log(sqft_lot)', SSEVals)
SSEVals = findSSEByColName('floors', SSEVals)
SSEVals = findSSEByColName('waterfront', SSEVals)
SSEVals = findSSEByColName('view', SSEVals)
SSEVals = findSSEByColName('grade', SSEVals)
SSEVals = findSSEByColName('yr_built', SSEVals)
SSEVals = findSSEByColName('lat', SSEVals)
SSEArr = SSEVals[[2]];
which(SSEArr==min(SSEArr))
SSEVals[[1]][which(SSEArr==min(SSEArr))]

cat("\nSTEP -01 : Conculsion: SSE is minimum for variable 'grade' so it is the best predictor, when we use single variable")
```
## #################################################
## ###### stepByStep Analysis - Step -01 #######
## ######  One variable predication - End ######
## #################################################

## #########################################################
## ###### stepByStep Analysis - Step -02 #######
## ###### Add more attributes to model - START ######
## #################################################


# we will also use R-squared to measure accuracy of model with addition of more variable
#R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. 0% indicates that the model explains none of the variability of the response data around its mean
```{r echo=TRUE}
# As in last phase, grade is bet to predict the price with single variable
# let us name is model01 & compute r_square for the model
# Linear Model
model01 <- lm(data=train,log(price)~grade)
summary(model01)
# R-Square of Model
r_squared_model01<-summary(model01)$r.squared
# RMSE of Model
predic_model01<-exp(predict(model01,interval='prediction',newdata=test)) 
RMSE_model01=sqrt(sum((predic_model01 - test$price)^2)/nrow(test)) 
```


# Now let us add more variable which has greater impact on the price prediction. 
# Let us call it model02. 
# Approach of attribute selection:
# I have selecetd log(sqft_living), bedrooms, bathrooms, grade, waterfront
# here I have tried to make mix-match of the variables which have strong/moderate correlation with price, but variable do not strong co-orelation between then
```{r echo=TRUE}
# Linear Model
model02<-lm(log(price)~log(sqft_living)+bedrooms+bathrooms+grade+waterfront,data=train)
summary(model02)
# R-Square of Model
r_squared_model02<-summary(model02)$r.squared
##compute RMSE for Model-02
predic_model02<-exp(predict(model02,interval='prediction',newdata=test)) 
RMSE_model02=sqrt(sum((predic_model02 - test$price)^2)/nrow(test)) 

cat("\nR-Squared for Model-02 is ",100*(r_squared_model02/r_squared_model01-1),"% better than Model-01.")
cat("\nSTEP -02 : Conculsion: \n Model-02 will predict the price better tha Model-01")
predic_model02<-exp(predict(model02,newdata=test)) 
```
## #################################################
## ###### stepByStep Analysis - Step -02 #######
## ###### Add more attributes to model - END ######
## #################################################

## #################################################
## ###### stepByStep Analysis - Step -03 #######
## ## new attributes using Residual approach ###
## ########## START ###########################
## #################################################

## If we plot residual vs. a variable (that is not used in the prediction) and if we see any recognizable patterns, 
## then it indicates that some of the variation in residual is due to non-used variable 
## therefore we should include it in our model to reduce the residual errors. 
## To calculate residuals, 
## we simply need to substract predic_model02 from the actual price.
```{r echo=TRUE}
##calculate residual for Model-02
residual_model02=test$price - predic_model02
## Residual vs. sqft_lot
plot(test$sqft_lot,residual_model02, col=(c("gold","darkgreen"))) 

## Residual vs. floors
plot(test$floors,residual_model02, col=(c("gold","darkgreen"))) 

## Residual vs. view
plot(test$view,residual_model02, col=(c("gold","darkgreen"))) 

## Residual vs. condition
plot(test$condition,residual_model02, col=(c("gold","darkgreen"))) 

## Residual vs. sqft_above
plot(test$sqft_above,residual_model02, col=(c("gold","darkgreen"))) 

## Residual vs. sqft_basement
plot(test$sqft_basement,residual_model02, col=(c("gold","darkgreen"))) 

## Residual vs. yr_built
plot(test$yr_built,residual_model02, col=(c("gold","darkgreen"))) 

## Residual vs. yr_renovated
plot(test$yr_renovated,residual_model02, col=(c("gold","darkgreen"))) 

## Residual vs. zipcode
#plot(test$zipcode,residual_model02, col=(c("gold","darkgreen"))) 

## Residual vs. lat
plot(test$lat,residual_model02, col=(c("gold","darkgreen"))) 

## Residual vs. long
plot(test$long,residual_model02, col=(c("gold","darkgreen"))) 

## Residual vs. sqft_living15
plot(test$sqft_living15,residual_model02, col=(c("gold","darkgreen"))) 

## Residual vs. sqft_lot15
plot(test$sqft_lot15,residual_model02, col=(c("gold","darkgreen"))) 

cat("\nSTEP -03 : Conculsion: After analzing scattered plot, we found that yr_built & lat are good candidate to predict the price of house")

```
## #################################################
## ###### stepByStep Analysis - Step -03 #######
## ## new attributes using Residual approach ###
## ########## End ###########################
## #################################################


## #################################################
## ###### stepByStep Analysis - Step -04 #######
## ##  Residual approach and get new model ####
## ########## STRAT ###########################
## #################################################

## Below two columns is being added
## 1: yr_built 
## 2:lat 
```{r echo=TRUE}
#Now create model03, which will include yr_built & lat and we will try to find model03 vs model02
model03<-lm(log(price)~log(sqft_living)+bedrooms+bathrooms+grade+waterfront+yr_built+lat,data=train)
summary(model03)

r_squared_model03<-summary(model03)$r.squared

cat("\nR-Squared for Model-03 is ",100*(r_squared_model03/r_squared_model02-1),"% better than Model-02.\nR-squared for Model-03 and Model-02 are:", r_squared_model03,"and", r_squared_model02, "respectively.So here Model-03 wins over other previous model.")

## RMSE for Model-03:
predic_model03<-exp(predict(model03,interval='prediction',newdata=test))
RMSE_model03=sqrt(sum((predic_model03 - test$price)^2)/nrow(test))

cat("\nRMSE for model02:",RMSE_model02,"\nRMSE for model03:",RMSE_model03)

cat("\nConclusion: RMSE for Model-02 is ",round(100*(RMSE_model02/RMSE_model03-1),2),"% more than Model-03. So Model-03 predicts the prices better.")

```
## #################################################
## ###### stepByStep Analysis - Step -04 #######
## ##  Residual approach and get new model ####
## ########## END ###########################
## #################################################


## #################################################
## ###### stepByStep Analysis - Step -05 #######
## ##  Tip/Formula: what you will look for ####
## ######### when you buy a house #############
## ##  enrich model with domain knowlege #####
## ################ START ##################
## #################################################

#log(price) seems to have good correlation with exp(bathrooms).
#log(price) seems to have good correlation with log(bedrooms), add 0.05 as it has 0 values.
#log(price) seems to have good correlation with log(lat). to reduce the noice and we will use lat-min(lat)+0.5, so that relative relation can be found. Similar approach has been taken for long
#log(price) seems to have good correlation with log(yr_renovated). to get more relevent data we wil try to compute the age of the building
#bedroom & bathroom can also play a big role when price of an house is being calculate, so i have use bedrooms*bathrooms as one of the variable to compute linear model
#grade & condition can also play a big role when price of an house is being calculate, so i have use log(grade)*exp(condition) as one of the variable to compute linear model
```{r echo=TRUE}
model05<-lm(log(price)~grade+log(sqft_living)+log(bedrooms+0.5)+exp(bathrooms)+waterfront+log(abs(lat-min(lat))+0.5)+log(abs(long-min(long))+0.05)+(zipcode*lat)+log(view+0.5)+condition+log(sqft_above+0.05)+log(sqft_basement+0.05)+log(sqft_lot15)+log(2015-yr_renovated+1)+(bedrooms*bathrooms)+(log(grade)*exp(condition))+(bedrooms*log(sqft_living))+(view*bedrooms),data=train)
summary(model05)

```

## Let's compute the root mean square error
```{r echo=TRUE}
## computing RMSE for Model-05
prediction05<-exp(predict(model05,interval='prediction',newdata=test)) 
RMSE_05=sqrt(sum((prediction05 - test$price)^2)/nrow(test))

cat("RMSE for model03:",RMSE_model03,"\nRMSE for model05:",RMSE_05)

cat("Conclusion: RMSE for Model-03 is ",round(100*(RMSE_model03/RMSE_05-1),2),"% more than Model-05, so here Model-05 is the clear winner.")
```

## Let's find the percentage of cases with less than 25% error.
## this process will give the confidence in the Model
```{r echo=TRUE}
errors <- prediction05[,'fit'] - test$price
hist(errors,col=(c("gold","darkgreen")))
cat("Conclusion: we can see that Histogram of error shows that possibility of error are less, which gives confidence in the variable selection for the model with current set of data")

rel_change = 1 - ((test$price - abs(errors)) / test$price)
##Now the percentage of cases with less than 25% error.
pred25 = table(rel_change<0.25)["TRUE"] / nrow(test)
pred25
cat("Conclusion: we can see 70%+ have 25% or less error.")

```
## #################################################
## ################ END ##################
## ###### stepByStep Analysis - Step -05 #######
## ##  Tip/Formula: what you will look for ####
## ######### when you buy a house #############
## ##  enrich model with domain knowlege #####
## #################################################


## #################################################
## ###### stepByStep Analysis - Step -06 #######
## ##  Get most accurate ######################
## ## Intercept & coefficient of Final Model ##
## ############# START ########################
## #################################################

## As above model is winner, 
## let us run the model on different set of test and training data and find the best RMSE
##  I Start with 
## Training-dataset = 60% 
## Testing-dataset = 40 % , 
## and increment Training-dataset by 1% data  
## and continue till Training-dataset reach to 95%.

```{r echo=TRUE}
set.seed(1)
#newhouseData <- subset(houseData, select  = c(price,bathrooms,sqft_living,grade,sqft_above, bedrooms, waterfront, lat, long, view, condition, sqft_basement, yr_renovated, sqft_lot15, zipcode))
newhouseData <- houseData
i=0.6
storage <- list(c(), c(), c(),c())
for(i in seq(from=0.60, to=0.95, by=0.01)){
  rn_train <- sample(nrow(newhouseData),floor(nrow(newhouseData)*i))
  train <- newhouseData[rn_train,colnames(newhouseData)]
  test <- newhouseData[-rn_train,colnames(newhouseData)]
  model <- lm(log(price)~grade+log(sqft_living)+log(bedrooms+0.5)+exp(bathrooms)+waterfront+log(abs(lat-min(lat))+0.5)+log(abs(long-min(long))+0.05)+(zipcode*lat)+log(view+0.5)+condition+log(sqft_above+0.05)+log(sqft_basement+0.05)+log(sqft_lot15)+log(2015-yr_renovated+1)+(bedrooms*bathrooms)+(log(grade)*exp(condition))+(bedrooms*log(sqft_living))+(view*bedrooms),data=train)
#summary(model)
  
  prediction <- round(exp(predict(model,interval='prediction',newdata = test)),0)
  train_prediction = fitted(model)
  train_rmse = sqrt(sum((train_prediction-train$price)^2)/nrow(train))
  test_rmse = sqrt(sum((prediction - test$price)^2)/nrow(test))
  
  storage[[1]]<-c(storage[[1]],i)
  storage[[2]]<-c(storage[[2]],test_rmse)
  storage[[3]]<-c(storage[[3]],train_rmse)
}

##find the LM with minimun training error
RMSE = storage[[3]]
minimumVal = min(RMSE)
minimumVal
indx = which(RMSE==min(RMSE))
indx
storage[[1]][indx]

cat("\nConclusion:  Minimum Training RMSE of Regression:",storage[[3]][indx],"\nRMSE of testing :",storage[[2]][indx], "\nTraining data Percentage:",storage[[1]][indx])

```
## #################################################
## ###### stepByStep Analysis - Step -06 #######
## ##  Get most accurate ######################
## ## Intercept & coefficient of Final Model ##
## ############# END ########################
## #################################################


## ##################################################
## ###### stepByStep Analysis - Step -07 ####
## ##Goal: Find out % data having error #####
## ## less than 25% for predicted price #####
## ## Give evidence that model has #########
## ## higer accuracy - END #################
## #################################################

## Now we come to a conclusion that 77% Training data provides the Minimum RMSE
## 1: SET training Data = 77% &
## 2: Get model with coeeficient & Intercept
## 3: Draw the error of Histogram to get confidence with model
## 4: Find out how many data have less than 25% of error
```{r}
rn_train <- sample(nrow(newhouseData),floor(nrow(newhouseData)*storage[[1]][indx]))
train <- newhouseData[rn_train,colnames(newhouseData)]
test <- newhouseData[-rn_train,colnames(newhouseData)]
modelXGen <- lm(log(price)~grade+log(sqft_living)+log(bedrooms+0.5)+exp(bathrooms)+waterfront+log(abs(lat-min(lat))+0.5)+log(abs(long-min(long))+0.05)+(zipcode*lat)+log(view+0.5)+condition+log(sqft_above+0.05)+log(sqft_basement+0.05)+log(sqft_lot15)+log(2015-yr_renovated+1)+(bedrooms*bathrooms)+(log(grade)*exp(condition))+(bedrooms*log(sqft_living))+(view*bedrooms),data=train)
summary(modelXGen)

predictionXGen <- round(exp(predict(modelXGen,interval='prediction',newdata = test)),0)
test_rmseXGen = sqrt(sum((predictionXGen - test$price)^2)/nrow(test))
errors <- predictionXGen[,'fit'] - test$price
hist(errors,col=(c("gold","darkgreen")))
rel_change = 1 - ((test$price - abs(errors)) / test$price)
##Now the percentage of cases with less than 25% error.
pred25 = table(rel_change<0.25)["TRUE"] / nrow(test)
pred25

cat("\nConclusion: Percent of data having less than 25% error:",pred25)
```
## ##################################################
## ###### stepByStep Analysis - Step -07 ####
## ##Goal: Find out % data having error #####
## ## less than 25% for predicted price #####
## ## Give evidence that model has #########
## ## higer accuracy - END #################
## #################################################

## Now Write the file 
```{r echo=TRUE}
#Now write the Real & Predicted price file for comparision
predictionXGen <- round(exp(predict(modelXGen,newdata = test)),0)
values <- (cbind("ID"=test$id,"Orginal Price"=test$price,"Predicted Price"=predictionXGen))
write.csv(values, file = "RealPriceVsPredictedAsStaticticalAnalysis.csv", row.names=FALSE)


```
## #################################################
## ###### stepByStep Analysis - End ########
## #################################################

## #################################################
## ####### stepAIC Analysis - START ##########
## #################################################

## Now Let perform feature selection using stepAIC
```{r}
houseData <- read.csv("file:///G:/Ryerson-BigData/capstone-R/CKME-136/data/kc_house_data.csv")
houseData$date<-NULL
houseData$id<-NULL
colnames(houseData)
```

##Let us now use the forward selection algorithm using stepAIC.
```{r echo=TRUE}
full = lm(price~.,data=houseData)
null = lm(price~1,data=houseData)
stepF = stepAIC(null, scope=list(lower=null, upper=full), direction='forward', trace=TRUE)
summary(stepF)

subsets = regsubsets(price~.,data=houseData,nbest=1,)
sub.sum = summary(subsets)
as.data.frame(sub.sum$outmat)

```


##Let us now use the backward selection algorithm using stepAIC.
```{r echo=TRUE}
full = lm(price~.,data=houseData)
null = lm(price~1,data=houseData)
stepF = stepAIC(full, direction= 'backward', trace=TRUE)
summary(stepF)

subsets = regsubsets(price~.,data=houseData,nbest=1,)
sub.sum = summary(subsets)
as.data.frame(sub.sum$outmat)

```

```{r echo=TRUE}
sapply(houseData, is.numeric)
houseDatas <- houseData[ , sapply(houseData, is.numeric)]
cor(houseDatas)
```

## Using Step AIC we got below variables needs to part of model
## 1:sqft_living 
## 2:lat 
## 3:grade 
## 4:yr_built 
## 5:waterfront 
## 6:bedrooms 
## 7:bathrooms 
## 8:zipcode 
## 9:long

```{r echo=TRUE}
#since floor is having very less contribution to price.we are not including floor in our model.
houseData$floors<-NULL
houseData$date<-NULL
colnames(houseData)
```

```{r echo=TRUE}
newhouseDataAIC <- subset(houseData, select  = c(price,sqft_living,lat,view,grade,yr_built,waterfront,bedrooms,bathrooms,zipcode,long))

set.seed(1)
i=0.6
storageAIC <- list(c(), c(), c(),c())
for(i in seq(from=0.6, to=0.9, by=0.01)){
  rn_train <- sample(nrow(newhouseDataAIC),floor(nrow(newhouseDataAIC)*i))
  train <- newhouseDataAIC[rn_train,colnames(newhouseDataAIC)]
  test <- newhouseDataAIC[-rn_train,colnames(newhouseDataAIC)]
  model<-lm(price~sqft_living+lat+view+grade+yr_built+waterfront+bedrooms+bathrooms+zipcode+long,data = train)
  prediction <- predict(model,interval='prediction',newdata = test)
  train_prediction = fitted(model)
  train_rmse = sqrt(sum((train_prediction-train$price)^2)/nrow(train))
  test_rmse = sqrt(sum((prediction - test$price)^2)/nrow(test))
  storageAIC[[1]]<-c(storageAIC[[1]],i)
  storageAIC[[2]]<-c(storageAIC[[2]],test_rmse)
  storageAIC[[3]]<-c(storageAIC[[3]],train_rmse)
 
}

##find the LM with minimun training error
RMSE = storageAIC[[3]]
minimumVal = min(RMSE)
minimumVal
indxAIC = which(RMSE==min(RMSE))
indxAIC
storageAIC[[1]][indxAIC]

cat("\nStepAIC Finding: Minimum Training RMSE of Regression:",storageAIC[[3]][indxAIC],"\nRMSE of testing :",storageAIC[[2]][indxAIC], "\nTraining data Percentage:",storageAIC[[1]][indxAIC])

```


## Now we come to a conclusion that 73% Training data provides the Minimum RMSE
## 1: SET training Data = 73% &
## 2: Get model with coeeficient & Intercept
## 3: Draw the error of Histogram to get confidence with model
## 4: Find out how many data have less than 25% of error
```{r}
set.seed(1)
rn_train <- sample(nrow(newhouseDataAIC),floor(nrow(newhouseDataAIC)*storage[[1]][indxAIC]))
train <- newhouseDataAIC[rn_train,colnames(newhouseDataAIC)]
test <- newhouseDataAIC[-rn_train,colnames(newhouseDataAIC)]
modelXGenAIC <- lm(price~sqft_living+lat+view+grade+yr_built+waterfront+bedrooms+bathrooms+zipcode+long,data = train)
summary(modelXGenAIC)

predictionXGenAIC <- predict(modelXGenAIC,interval='prediction',newdata = test)
test_rmseXGenAIC = sqrt(sum((predictionXGen - test$price)^2)/nrow(test))
errors <- predictionXGenAIC[,'fit'] - test$price
hist(errors,col=(c("gold","darkgreen")))
rel_change = 1 - ((test$price - abs(errors)) / test$price)
##Now the percentage of cases with less than 25% error.
pred25AIC = table(rel_change<0.25)["TRUE"] / nrow(test)
pred25AIC

cat("\nConclusion:percent of data having less than 25% error:",pred25)
```

## #################################################
## ####### stepAIC Analysis - END ##########
## #################################################



## #################################################
## ####### Boruta Analysis - START ##########
## #################################################

# USING BORUTA for features selection and finally making model with the selected attributes
```{r echo=TRUE}
set.seed(1)
boruta.train <- Boruta(price ~., data = houseData , doTrace = 2,ntree = 500)
print(boruta.train)
plot(boruta.train, xlab = "", xaxt = "n")
Boruta.Short <- Boruta(price ~ ., data = houseData, maxRuns = 12)
```

#Start calculating RMSE
```{r echo=TRUE}
newhouseDataBoruta <- houseData

set.seed(1)
i=0.6
storageBoruta <- list(c(), c(), c(),c())
for(i in seq(from=0.6, to=0.9, by=0.01)){
  rn_train <- sample(nrow(newhouseDataBoruta),floor(nrow(newhouseDataBoruta)*i))
  train <- newhouseDataBoruta[rn_train,colnames(newhouseDataBoruta)]
  test <- newhouseDataBoruta[-rn_train,colnames(newhouseDataBoruta)]
  model <- lm(formula = price~.,data=train)
  prediction <- predict(model,interval='prediction',newdata = test)
  train_prediction = fitted(model)
  train_rmse = sqrt(sum((train_prediction-train$price)^2)/nrow(train))
  test_rmse = sqrt(sum((prediction - test$price)^2)/nrow(test))
  storageBoruta[[1]]<-c(storageBoruta[[1]],i)
  storageBoruta[[2]]<-c(storageBoruta[[2]],test_rmse)
  storageBoruta[[3]]<-c(storageBoruta[[3]],train_rmse)
}

##find the LM with minimun training error
RMSE = storageBoruta[[3]]
minimumVal = min(RMSE)
minimumVal
indxBoruta = which(RMSE==min(RMSE))
indxBoruta
storageBoruta[[1]][indxBoruta]

cat("\nBourta Finding: Minimum Training RMSE of Regression:",storageBoruta[[3]][indxBoruta],"\nRMSE of testing :",storageBoruta[[2]][indxBoruta], "\nTraining data Percentage:",storageBoruta[[1]][indxBoruta])

```


## Now we come to a conclusion that 73% Training data provides the Minimum RMSE
## 1: SET training Data = 73% &
## 2: Get model with coeeficient & Intercept
## 3: Draw the error of Histogram to get confidence with model
## 4: Find out how many data have less than 25% of error
```{r echo=TRUE}
rn_train <- sample(nrow(newhouseDataBoruta),floor(nrow(newhouseDataBoruta)*storageBoruta[[1]][indxBoruta]))
train <- newhouseDataBoruta[rn_train,colnames(newhouseDataBoruta)]
test <- newhouseDataBoruta[-rn_train,colnames(newhouseDataBoruta)]
modelXGenBoruta <- lm(price~.,data = train)
summary(modelXGenBoruta)

predictionXGenBoruta <- predict(modelXGenBoruta,interval='prediction',newdata = test)
test_rmseXGenBoruta = sqrt(sum((predictionXGenBoruta - test$price)^2)/nrow(test))
errors <- predictionXGenBoruta[,'fit'] - test$price
hist(errors,col=(c("gold","darkgreen")))
rel_change = 1 - ((test$price - abs(errors)) / test$price)
##Now the percentage of cases with less than 25% error.
pred25Boruta = table(rel_change<0.25)["TRUE"] / nrow(test)
pred25Boruta
cat("\nConclusion:percent of data having less than 25% error:",pred25Boruta)

```
## #################################################
## ####### Boruta Analysis - END ##########
## #################################################




## ##################################################
## ###### Summary Of ########################
## ###### StepAIC  ##########################
## ########## Vs  ###########################
## ###### Boruta  ###########################
## ######### Vs  ############################
## ###### Step By Step Analysis #############
## ################# START ##################
## ##################################################
##Below is the Summary of three models
##1: Model by StepByStep Analysis  ##2: stepAIC ##3: Boruta

```{r echo=TRUE}

cat("\nModel Category\t\t\tRMSE\t\t %ofData-havig-less-than 25% error","\nModel by StepByStep Analysis\t\t\t",test_rmseXGen,"\t\t",pred25,"\nstepAIC\t\t\t",test_rmseXGenAIC,"\t\t",pred25AIC,"\nBoruta\t\t\t",test_rmseXGenBoruta,"\t\t",pred25Boruta,"\n\nConclusion: Model by StepByStep Analysis with RMSE",test_rmseXGen,"%ofData-havig-less-than 25% error",pred25," is Winner.")

```
## #################################################
## ###### Summary Of #######################
## ###### StepAIC  ###########################
## ########## Vs  ###########################
## ###### Boruta  ###########################
## ######### Vs  ###########################
## ###### Step By Step Analysis ##############
## ################# END ###################
## #################################################


## ########################################
## ###### Step By Step Analysis ###########
## ###### Different Model Comparison ######
## ########### at #########################
## ###### best data partition #############
## ############ START #####################
## ########################################

## Now let us verify RMSE for each Model derived due to stepByStep analysis 
## I would like to find out the winner among all the model generated due to StepByStep is winner

```{r echo=TRUE}
#Get The data divided 
set.seed(1)
rn_train <- sample(nrow(newhouseData),floor(nrow(newhouseData)*storage[[1]][indx]))
train <- newhouseData[rn_train,colnames(newhouseData)]
test <- newhouseData[-rn_train,colnames(newhouseData)]

#Model01-Start
model01 <- lm(data=train,log(price)~grade)
predic_model01<-exp(predict(model01,interval='prediction',newdata=test)) 
RMSE_model01=sqrt(sum((predic_model01 - test$price)^2)/nrow(test)) 

#Model02-Start
model02<-lm(log(price)~log(sqft_living)+bedrooms+bathrooms+grade+waterfront,data=train)
predic_model02<-exp(predict(model02,interval='prediction',newdata=test)) 
RMSE_model02=sqrt(sum((predic_model02 - test$price)^2)/nrow(test)) 


#Model03-Start
model03<-lm(log(price)~log(sqft_living)+bedrooms+bathrooms+grade+waterfront+yr_built+lat,data=train)
predic_model03<-exp(predict(model03,interval='prediction',newdata=test))
RMSE_model03=sqrt(sum((predic_model03 - test$price)^2)/nrow(test))

##modelXGen Start
modelXGen <- lm(log(price)~grade+log(sqft_living)+log(bedrooms+0.5)+exp(bathrooms)+waterfront+log(abs(lat-min(lat))+0.5)+log(abs(long-min(long))+0.05)+(zipcode*lat)+log(view+0.5)+condition+log(sqft_above+0.05)+log(sqft_basement+0.05)+log(sqft_lot15)+log(2015-yr_renovated+1)+(bedrooms*bathrooms)+(log(grade)*exp(condition))+(bedrooms*log(sqft_living))+(view*bedrooms),data=train)
predictionXGen <- round(exp(predict(modelXGen,interval='prediction',newdata = test)),0)
test_rmseXGen = sqrt(sum((predictionXGen - test$price)^2)/nrow(test))


cat("Below are the finding when train-data:",storage[[1]][indx]," and Testing-Data:", (1-storage[[1]][indx]))
cat("\nRMSE for model01:",RMSE_model01,"\nRMSE for model02:",RMSE_model02,"\nRMSE for model03:",RMSE_model03,"\nRMSE for modelXGen:", test_rmseXGen)

cat("\nFindings: RMSE for Model-01 is ",round(100*(RMSE_model01/RMSE_model02-1),2),"% more than Model-02. So Model-02 predicts the prices better.")
cat("\nFindings: RMSE for Model-02 is ",round(100*(RMSE_model02/RMSE_model03-1),2),"% more than Model-03. So Model-03 predicts the prices better.")
cat("\n\n Conclusion: RMSE for Model-03 is ",round(100*(RMSE_model03/test_rmseXGen-1),2),"% more than Model-XGen.\n\nSo Model-xGen predicts the prices better even at ", storage[[1]][indx] , " percentage of training data.")


```
## ########################################
## ###### Step By Step Analysis ###########
## ###### Different Model Comparison ######
## ########### at #########################
## ###### best data partition #############
## ############ END #####################
## ########################################

